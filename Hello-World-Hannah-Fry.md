### "Hello World" by Hannah Fry

## Introduction
- Algorythms (or solutions in a wider sense) are not inherently good or bad
- "Bad" algorythms often had ill intentions behind them or incompetence

## Power
- We use AI, ML and data in disciplines like medicine, cars, judicial systems without fully understanding where their decisions come from
- There is some evidence suggesting that humans trust technology, possibly because it's *technology* after all, without engaging critical thinking
  - e.g. when GPS leads us to a lake and we drive into it without question
  - or when our opinions can be shaped depending on which results we get first in the Internet browser - which is **dangerous** and very **powerful**
  - algorithms have a, not always deserved, sense of *authority*

- transparency is key, especially when real lives are at stake
  - How can you be transparent with a ML algorithm or neural network? 
  - When challenging an algorithm you need to be more convincing, sure and courageous in your convictions, as you're fighting against people's trust in the machine, not just the machine
- While we're likely to over-trust an algorithm, when we know that it can be faulty, we react by dismissing the algorithm more than we need to
  - it's called *algorithm aversion* 
- We need to find balance between complete trust and complete aversion to algorithms.

## Data
- using an app or a service creates an exchange, a person gets the service, but gives their data
- We're not always aware of the long-terms implications of such an exchange
  - E.g. Tesco started Clubcard by giving out points in return for purchase info. While uncomplicated and limited at first, it quickly became sopisticated and robust. 
- Plantir - data broker in SV. Sells personal data and resells them with profit.
  - data is anonymised, but the sheer volume makes it possible to infer someone's identity. However, often, someone's name can be in the URL which makes it extremely easy to de-anonymise their whole browsing history
  - 'cookies' - a piece of data generated by a visited website which is placed on a user device and stores information on the user's activity
- Cambridge Analytica
  - asked the users of Facebook to take personality tests, handing both their Like and browsing history with their test results
  - ads were tailored based on the personality, e.g. more actioanble for extrovers and more calm and serene for introverts
  - work for the Trump election campaign - targeted people who seemed 'persuadable' with personalised ads, delivered ads dressed as journalism
  - studies suggests that we're actually pretty good in ignoring false advertisement, however a small change in behaviour may still be powerful enough to influence important events

## Justice

- lack of consistency between judges and also between their own decisions
- using  data in courtrooms is tricky - you need to balance individualised justice and ensure consistency - previous implementation struggled with the balance (US federal law vs Scottish courts)
- it's tricky to find characteristics on which to base the algorithm, Ernest Burgess used labels, like 'farmboy' to predict recidivism, which doesn't apply to modern city criminals. He was also critised for using irrelevant characteristics and base his algorithm on opinion. However, the predictive power of his model was so strong it became standard to use. It's even used to this day, with some changes.
- Alorithms used in current judicial systems tend to outperform the judges in several categories, such as reoffending, giving bail, which criminals to release. This resulted in decrease in prison populations and recidivism rates
  - COMPAS - provides a risk score for each defendant. Code not available to the public. Answers to simplistic questions used to make a decision about multi-faceted cases.COMPAS is used by judges as aid in making a verdict, often replacing their opinion. This results in removing the human-aspect of the case. 
  - When judges use COMPAS's decision, and deny bail, there is no way of knowing whether the model predicted accurately that the defendant would reoffend. 
  - COMPAS uses statistical inference to predict whether someone would reoffend. Due to the fact that some populations have higher records of crime, such as men or people of colour, people that share these characteristics are much more likely to be wrongfully classed as 'reoffenders'. The algorithm can perpetuate the inequalities of the past based on mathematical formula. Which is technicall what you would expect, but in reality not an ideal solution. 

> "Until all groups are arrested at the same rate, this kind of bias is a mathermatical certainty."

- Algorithms will show biases of the data given to them. If most pictures of offenders depict black people, then a Google search of an 'offender' will return an array of black people. 
- Although there is evidence that using algortihms helps, we need to challenge them and the data used for their training.
- **anchoring effect** - we struggle with putting numerical values to things and find it easier to make comparisons rather than coming up with the value ourselves. This bias is used in advertising, when stores will achor certain values related to products in our mind and use it to increase sales. E.g. (an inflated) limit on the number of items doesn't mean that this item is so popular a limit is needed. Instead it gives us a (wrong) idea of how many items people buy on average, so we end up buying more.


## Medicine

- neural networks are used in medicine for image recognition. The difficulty is not only to recognise a picture with an illness in it, but to ignore pictures that don't have any ill tissue. 
  - the more pictures we feed into the neural network, the more accurate it becomes
  - the ways to recognise an object might differ significantly from the way humans recognise objects. E.g. wolves were distinguished from dogs if snow was present in the background.
  - issue in refining an algorithm - trade-off between *sensitivity* and *specificity*. 
> If you decided to prioritise the complete elimination of false negatives, your algorithm would flag every single breast it saw as suspicious. That would score 100 percent sensitivity (...). It would also mean an awful lot of perfectly healthy people undergoing unnecessary treatment. Or if you decided to prioritise the complete elimination of false positives. Your algorithm would wave everyone through as healthy, thus earning 100 percent score on specificity. (page 88)
 
- humans (specifically pathologists) don't tend to have problems with sensitivity (false positives) but they strrugle with sensitivity (false negatives)
- algorithms (e.g. neural networks used at CAMELYON16) struggled with false positives (specificity)
- combining human and machine repsonses gives the best outcomes
- *The Nun Study* - a longitudinal study which followed nuns throughout their life to investigate the causes of Alzheimer's desease. The nuns were all similar age, had similar live conditions, didn't smoke or drink and didn't have children making them a perfect control group for the experiment
  - apart from regular data which helped uncover some insights, the reserachers also had access to their entry essays to the Sisterhood. 
  - Researchers uncovered that the language, how articulate the woman was, and sentence complexity used in the essay were a great predictor on the later onset of Alzheimer's desease and decline in cognitive ability
  - This suggests that subtle hints in our current behaviour can indicate an onset of disease decades later.
- In many areas of medicine, doctors are good in detecting anomalies in our bodies but not good in predicting how these anomalies develop later on.
- algorithms in the NL were trained to do that and detected that the best predictor in how the anomaly will develop is by looking at changes in adjacent tissues
- there are also algorithms that have been successfuly trained to doagnose certain diseases from images or test results. Such as diabetic rethinopathy, which is a preventable illness caused by affected blood vessels in the light-sensitive areas of the eye and which mostly affects people in India. 
- attempts were made to create an all-encompassing algorithm, called Watson, that can diagnose any disease using patients records, exams and interviews
  - the projects largely failed
  - Watson diagnosed certain diseases and a few patients when the doctors didin't, but it was never able to go into mainstream production
  - Watson struggled with chain of events, a symptom could be caused by an illness, but also could be a result of something else, that was a result of something else. This was very difficult for the algorithm to logically discover
  - training of Watson was also an issue - experts may take years to learn how to correctly label the data and observations. Does 'cold' mean a temperature or a flu? 
- DeepMind (Google) - signed a contract with NHS to access their patient data to make an app that would help in the diagnosis of acute kidney injuries
  - project failed because data wasn't good enough
  - none of the patients was asked for constent to share their personal data with a private company
- opening our medical records to algorithms poses some great benefits but also risks
  - risks - we lose privacy, our records could be accessed by companies and we could be denied insurance, doctors may make assumptions about our lifestyle and prioritise people who don't smoke, eat healthier for surgeries
  - benefits - better prevention, data for algorithm to train on, developing algorithms that can diagnose faster and more accurately, automation and faster access to healthcare
- DNA testing, such as 23andMe - the long-term goal of the company was to collect DNA rather than sell testing kits. The information gathered from the tests may be harmful to individuals in the long-term. For example, in the US people who tested their DNA for Parkinson's or Alzheimer's may be denied insurance. In the UK insurers are allowed to take your results for Huntington's desease if the cover is over Â£500,000
- If a diagnostic machine capable of recommending treatments for all disease was made, who would it prioritise? The individual, or the population?
  - there may be times when it has to choose
  - would this algorithm prescribe you antibiotics to shorten your illness, or prioritise increasing global antibiotic resistance and not prescribe you anything?
  - would the algorithm pragmatically refer you for further tests, or would it prioritise long queues and waiting times and send you home without any additional tests?
  - the machine will prioritise whatever it is told and the goal might be different for a pharmaceutical company, an insurer or a doctor.
- there is always balance to consider in healthcare algorithms
  - privacy vs public good
  - individual vs population
  - conflicting challenges 
  - conflicting priorities  

## Cars